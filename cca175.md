#### SparkSession
+ You control your Spark Application through a driver process called the SparkSession.
 
 `res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@...`

#### RDD

#### DataFrames


#### DataSet

#### DataFrames Versus Datasets
+ **untyped** DataFrames and the **typed** Datasets
+ To say that DataFrames are untyped is aslightly inaccurate; they have types, but Spark maintains them completely and only checks whether those types line up to those specified in the
  schema at ***runtime***
+  Datasets, on the other hand, check whether types conform to the specification at
  compile ***time***
+  Datasets are only available to Java Virtual Machine (JVM) – based languages (Scala
  and Java) and we specify types with case classes or Java beans.
+ DataFrames are simply Datasets of Type **Row**(The ***Row*** type is Spark’s internal representation of its optimized inmemory format for computation,JVM datatypes cause cause high **GC** and **instantiation costs**)

#### Columns
+ Columns represent a simple type like an integer or string, a complex type like an array or map, or a
     null value, Spark Column types are as columns in a table
     
#### Rows
+ A row is nothing more than a record of data. Each record in a DataFrame must be of type Row

#### Spark Types
+ Spark types are internal type representations

In Scala
> ```scala
> import org.apache.spark.sql.types._
> val b = ByteType
>```

In Java

> ```java
> import org.apache.spark.sql.types.DataTypes;
> ByteType x = DataTypes.ByteType;
>```

#### Spark Types in Scala

| DataType | Value type in Scala  |API to access or create a data type|
| -------- | ----------- |--------|
|ByteType|Byte|ByteType|
|ShortType|Short|ShortType|
|IntegerType|Int|IntegerType|
|LongType|Long|LongType|
|FloatType|Float|FloatType|
|DoubleType|Double|DoubleType|
|DecimalType|java.math.BigDecimal|DecimalType|
|StringType|String|StringType|
|BinaryType|Array[Byte]|BinaryType|
|BooleanType|Boolean|BooleanType|
|TimestampType|java.sql.Timestamp|TimestampType|
|DateType|java.sql.Date|DateType|
|ArrayType|scala.collection.Seq|ArrayType(elementType, [containsNull]). Note: The default value of containsNull is true.|
|MapType|scala.collection.Map|MapType(keyType, valueType, [valueContainsNull]). Note: The default value of valueContainsNull is true.|
|StructType|org.apache.spark.sql.Row|StructType(fields). Note: fields is an Array of StructFields. Also, fields with the same name are not allowed.|
|StructField|The value type in Scala of the data type of this field (for example, Int for a StructField with the data type IntegerType)|StructField(name, dataType, [nullable]). Note: The default value of nullable is true.|


#### Overview of Structured API Execution
<img src="https://github.com/anirudhreddy92/cca175scala/blob/master/execution%20plan.PNG" height="250" width="75%">

+ Write DataFrame/Dataset/SQLCode.
+ If valid code, Spark converts this to a Logical Plan.
+ Spark transforms this Logical Plan to a Physical Plan, checking for optimizations along the
way.
+ Spark then executes this Physical Plan (RDD manipulations) on the cluster.


#### Logical Planning
![](https://github.com/anirudhreddy92/cca175scala/blob/master/logical%20plan.PNG)
+ unresolved because
  although your code might be valid, the tables or columns that it refers to might or might not exist.
  Spark uses the catalog, a repository of all table and DataFrame information, to resolve columns and
  tables in the analyzer.
+ The analyzer might reject the unresolved logical plan if the required table or
  column name does not exist in the catalog. If the analyzer can resolve it, the result is passed through
  the Catalyst Optimizer, a collection of rules that attempt to optimize the logical plan by pushing down
  predicates or selections.
+  Packages can extend the Catalyst to include their own rules for domainspecific
  optimizations.

#### Physical Planning
![](https://github.com/anirudhreddy92/cca175scala/blob/master/physical%20plan.PNG)
+ The physical plan, often called a Spark plan, specifies how the logical plan will execute on
  the cluster by generating different physical execution strategies and comparing them through a cost
  model
+  An example of the cost comparison might be choosing how to
  perform a given join by looking at the physical attributes of a given table (how big the table is or how
  big its partitions are).
+ Physical planning results in a series of RDDs and transformations. This result is why you might have
  heard Spark referred to as a compiler—it takes queries in DataFrames, Datasets, and SQLand
  compiles them into RDD transformations for you.


